{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import librosa.display\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "from tqdm.auto import tqdm\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = wandb.util.generate_id()\n",
    "wandb.init(\n",
    "    project=\"audio-classification\",\n",
    "\n",
    "    config={\n",
    "        \"learning_rate\": 5e-4,\n",
    "        \"epochs\": 1100,\n",
    "        \"batch_size\": 16,\n",
    "        \"architecture\": \"CoAtNet\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keystroke(y, sr, keystroke_interval=0.6, energy_threshold=0.0015):\n",
    "    frame_size = int(sr * keystroke_interval)\n",
    "    hop_length = int(frame_size / 2)\n",
    "\n",
    "    energy = librosa.feature.rms(y=y,\n",
    "                                 frame_length=frame_size,\n",
    "                                 hop_length=hop_length)[0]\n",
    "    keystroke_mask = (energy > energy_threshold).astype(int)\n",
    "    onset_frames = librosa.onset.onset_detect(y=y,\n",
    "                                              sr=sr,\n",
    "                                              units='time')\n",
    "    onset_frames = (onset_frames * sr / hop_length).astype(int)\n",
    "\n",
    "    onset_frames_keystroke = onset_frames[keystroke_mask[onset_frames].astype(\n",
    "        bool)]\n",
    "    keystrokes = []\n",
    "\n",
    "    for onset_frame in onset_frames_keystroke:\n",
    "        start_sample = onset_frame * hop_length\n",
    "        end_sample = start_sample + frame_size\n",
    "        keystrokes.append(y[start_sample:end_sample])\n",
    "\n",
    "    return keystrokes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mel_spectrograms(keystrokes, sr, n_mels, hop_length, n_fft):\n",
    "    mel_spectrograms = []\n",
    "\n",
    "    for keystroke in keystrokes:\n",
    "        mel_spectrogram = librosa.feature.melspectrogram(y=keystroke,\n",
    "                                                         sr=sr,\n",
    "                                                         n_mels=n_mels,\n",
    "                                                         hop_length=hop_length,\n",
    "                                                         n_fft=n_fft)\n",
    "        mel_spectrogram = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
    "        mel_spectrograms.append(mel_spectrogram)\n",
    "\n",
    "    return mel_spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_MAPPING = {\n",
    "    'A': 0, 'B': 1, 'C': 2, 'D': 3,\n",
    "    'E': 4, 'F': 5, 'G': 6, 'H': 7,\n",
    "    'I': 8, 'J': 9, 'K': 10, 'L': 11,\n",
    "    'M': 12, 'N': 13, 'O': 14, 'P': 15,\n",
    "    'Q': 16, 'R': 17, 'S': 18, 'T': 19,\n",
    "    'U': 20, 'V': 21, 'W': 22, 'X': 23,\n",
    "    'Y': 24, 'Z': 25, '0': 26, '1': 27,\n",
    "    '2': 28, '3': 29, '4': 30, '5': 31,\n",
    "    '6': 32, '7': 33, '8': 34, '9': 35,\n",
    "}\n",
    "\n",
    "\n",
    "class KeystrokeDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, audio_path, n_mels=64, hop_length=256, n_fft=1024, keystroke_interval=0.6, energy_threshold=0.0015):\n",
    "        self.audio_path = audio_path\n",
    "        self.n_mels = n_mels\n",
    "        self.hop_length = hop_length\n",
    "        self.n_fft = n_fft\n",
    "        self.keystroke_interval = keystroke_interval\n",
    "        self.energy_threshold = energy_threshold\n",
    "\n",
    "        self.mel_spectrograms = []\n",
    "        self.labels = []\n",
    "\n",
    "        for f in os.listdir(self.audio_path):\n",
    "            if f.endswith('.wav'):\n",
    "                wav_file = os.path.join(self.audio_path, f)\n",
    "\n",
    "                y, sr = librosa.load(wav_file, mono=True)\n",
    "\n",
    "                keystrokes = extract_keystroke(y,\n",
    "                                               sr,\n",
    "                                               self.keystroke_interval,\n",
    "                                               self.energy_threshold)\n",
    "\n",
    "                mel_spectrograms = get_mel_spectrograms(keystrokes,\n",
    "                                                        sr,\n",
    "                                                        self.n_mels,\n",
    "                                                        self.hop_length,\n",
    "                                                        self.n_fft)\n",
    "                label = f.split('_')[0]\n",
    "                numerical_label = CLASS_MAPPING.get(label, -1)\n",
    "\n",
    "                self.mel_spectrograms.extend(mel_spectrograms)\n",
    "                self.labels.extend([numerical_label] * len(mel_spectrograms))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.mel_spectrograms)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        mel_spectrogram = self.mel_spectrograms[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        max_length = 52\n",
    "        mel_spectrogram = torch.tensor(mel_spectrogram)\n",
    "        mel_spectrogram = F.pad(\n",
    "            mel_spectrogram, (0, max_length - mel_spectrogram.shape[1]))\n",
    "\n",
    "        return mel_spectrogram, torch.tensor(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoAtNet(nn.Module):\n",
    "    def __init__(self, n_classes=36):\n",
    "        super(CoAtNet, self).__init__()\n",
    "\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=(3, 3),\n",
    "                      stride=(1, 1), padding=(1, 1)),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)),\n",
    "        )\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=64, nhead=8)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            encoder_layer, num_layers=2)\n",
    "\n",
    "        self.fc = nn.Linear(64, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(f\"Length input: {len(x)}\")\n",
    "        # print(f\"Before first layer: {x.shape}\")\n",
    "        x = self.conv_layers(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1, x.size(1)).permute(1, 0, 2)\n",
    "        # print(f\"After reshape: {x.shape}\")\n",
    "\n",
    "        x = self.transformer_encoder(x)\n",
    "        # print(f\"After transformer:{x.shape}\")\n",
    "\n",
    "        x = x.permute(1, 0, 2)\n",
    "\n",
    "        x, _ = torch.max(x, dim=1)\n",
    "        # x = x.view(x.size(0), -1)\n",
    "        # print(f\"Before FC: {x.shape}\")\n",
    "\n",
    "        x = self.fc(x)\n",
    "        # print(f\"Output shape: {x.shape}\")\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "AUDIO_PATH = 'data/Phone_Recording/'\n",
    "\n",
    "dataset = KeystrokeDataset(AUDIO_PATH)\n",
    "train_set, test_set = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "train_loader = DataLoader(train_set, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=16, shuffle=False)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = CoAtNet().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
    "\n",
    "n_epochs = 1100\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    for inputs, labels in tqdm(train_loader):\n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        reshaped_inputs = inputs.unsqueeze(1)\n",
    "        output = model(reshaped_inputs)\n",
    "\n",
    "        loss = criterion(output, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch {epoch + 1}/{n_epochs}, Loss: {loss.item():.4f}')\n",
    "    wandb.log({\"Epoch\": epoch, \"loss\": loss.item()})\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for inputs, labels in test_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                reshaped_inputs = inputs.unsqueeze(1)\n",
    "                output = model(reshaped_inputs)\n",
    "                _, predicted = torch.max(output.data, 1)\n",
    "\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "            print(f'Accuracy: {100 * correct / total:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
